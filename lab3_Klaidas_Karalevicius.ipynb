{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNk4mRv/AuB7xMy032rvRce",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KlaidasKaralevicius/NLP_lab_3/blob/main/lab3_Klaidas_Karalevicius.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Notice** (prie paleidžiant notebook)\n",
        "Apmokant modelį mažiau nei 5 minutes sumarizacija pateikia duoto teksto pradžią (žodis į žodį) tik sustoja anksčiau (apie 40 žodžių). Kad gauti patenkinamą rezultatą modelį reikėjo apmokyti and daugiau epochų (8 epochų davė patenkinamą rezultatą) todėl, kad atkartoti rezultatą reikia įkelti ilgiau apmokytą modelį. Instrukcija:\n",
        "\n",
        "\n",
        "*   Atkomentuoti *!gdown* komandos bloką\n",
        "*   Užkomentuoti *fit* komandos bloką\n",
        "*   Reikiamus blokus galima rasti -> CTRL+F ir įvesti 'pointer'\n",
        "\n",
        "**P.S.** Įkeliant jau apmokytą modelį užtenka paleisti tik 3 paskutinius notebook blokus (atkomentavus *!gdown*), nereikia nieko komentuoti, įkeldinėti ar tokenizuoti pradinių duomenų (užtenka naudoti CPU)."
      ],
      "metadata": {
        "id": "frAFMmdrXjLe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarisation by finetuning"
      ],
      "metadata": {
        "id": "nRGb-gzZXaZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sumarizacijos užduočiai reikia transformerio kuris turi ir enkoderį ir dekoderį, nes reikia sukurti naują tekstą sumarizacijai. Dėl to pasirinkau modelį kuris turi šias dvi dalis - BART (base model). Iš reikiamų modelių jis dažniausiai buvo paminėtas skirtinguose šaltiniuose. Iš pradžių buvo panaudotas didesnis BART modelis (bart-large-cnn), bet į jį galima paduoti tik labai mažus duomenų kiekis, kitu atveju visa RAM atmintis užsipildo ir notebook lūžta."
      ],
      "metadata": {
        "id": "t4SJ2qXmsIX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Duomenims fine-tunning užduočiai pasirinkau cnn_dailymail, iš pradžių pasirinkau XSum, bet jis buvo didesnis todėl ilgiau siuntėsi, apmokymui bus naudojama tik maža duomenų dalis, todėl neverta siūsti daugiau duomenų jei ne visi bus naudojami (duomenų paruošimas užims mažiau laiko)."
      ],
      "metadata": {
        "id": "WioQbNY8wOy-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelio tikslumui tikrinti panaudotas rouge_score, jis palygina gautą tekstą su testavimo tekstu ir tikrina kiek panašių n-gramų jie turi."
      ],
      "metadata": {
        "id": "553dyYy15ymg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import dependencies"
      ],
      "metadata": {
        "id": "ZeRhQ2uoZ6PC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q datasets transformers rouge_score evaluate nltk\n",
        "from datasets import load_dataset\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, get_scheduler\n",
        "import evaluate\n",
        "from accelerate import Accelerator\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import random\n",
        "random.seed(22)"
      ],
      "metadata": {
        "id": "YRrjvkbsqZ41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Įkelti duomenis ir tokenizuoti duomenis (naudoti tik duomenų mažą dalį)"
      ],
      "metadata": {
        "id": "YUZoBxaQaTSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", trust_remote_code=True)\n",
        "\n",
        "model_name = 'facebook/bart-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "D7438jB0Z_bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = 2000\n",
        "# iš visų duomenų pasirinkti tik atsitiktinius 2000, kad pagreitinti apmokymą\n",
        "small_dataset_train = dataset['train'].shuffle().select(range(n_samples))\n",
        "small_dataset_eval = dataset['test'].shuffle().select(range(n_samples))"
      ],
      "metadata": {
        "id": "59EZtvO3XZVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_length = 512\n",
        "max_target_length = 32\n",
        "\n",
        "def preprocess_function(data):\n",
        "  model_inputs = tokenizer(\n",
        "      data['article'],\n",
        "      max_length = max_input_length,\n",
        "      truncation = True,\n",
        "  )\n",
        "  labels = tokenizer(\n",
        "      data['highlights'],\n",
        "      max_length = max_target_length,\n",
        "      truncation = True\n",
        "  )\n",
        "  model_inputs['labels'] = labels['input_ids']\n",
        "  return model_inputs"
      ],
      "metadata": {
        "id": "hZOBI5TlahI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizuojami duomenys\n",
        "tokenized_train_dt = small_dataset_train.map(preprocess_function, batched=True)\n",
        "tokenized_eval_dt = small_dataset_eval.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "pb0HFH8qaia0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paruošti modelį ir duomenis"
      ],
      "metadata": {
        "id": "LpuWTo__akYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_score = evaluate.load('rouge')\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "833T-ulQakgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# iš tokenizuotų duomenų išmetami pradiniai stulpelių pavadinimai\n",
        "tokenized_train_dt = tokenized_train_dt.remove_columns(\n",
        "    small_dataset_train.column_names\n",
        ")\n",
        "tokenized_eval_dt = tokenized_eval_dt.remove_columns(\n",
        "    small_dataset_eval.column_names\n",
        ")"
      ],
      "metadata": {
        "id": "35-d6hXfaw80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_dt.set_format(\"torch\")\n",
        "tokenized_eval_dt.set_format(\"torch\")"
      ],
      "metadata": {
        "id": "lgLsCfcKax_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# paruošiami duomenys įkelimui į modelio apmokymą ir testavimą\n",
        "batch_size = 24\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_train_dt,\n",
        "    shuffle = True,\n",
        "    collate_fn = data_collator,\n",
        "    batch_size = batch_size,\n",
        ")\n",
        "eval_dataloader = DataLoader(\n",
        "    tokenized_eval_dt,\n",
        "    collate_fn = data_collator,\n",
        "    batch_size = batch_size,\n",
        ")"
      ],
      "metadata": {
        "id": "HVU-f5F2ayHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "accelerator = Accelerator()\n",
        "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, eval_dataloader\n",
        ")"
      ],
      "metadata": {
        "id": "mCjXI5qHa2Hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# paruošiami parametrai modelio apmokymui\n",
        "num_train_epochs = 1\n",
        "num_update_steps_per_epoch = len(train_dataloader)\n",
        "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer = optimizer,\n",
        "    num_warmup_steps = 0,\n",
        "    num_training_steps = num_training_steps,\n",
        ")"
      ],
      "metadata": {
        "id": "O-NuMXb2a_GY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelio apmokymas ir testavimas"
      ],
      "metadata": {
        "id": "ggkOaezTa_A2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# paruošiama funkciją modelio testavimui\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [label.strip() for label in labels]\n",
        "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
        "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
        "    return preds, labels"
      ],
      "metadata": {
        "id": "SDyP9c5obDti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"/content/bart-base-finetuned\""
      ],
      "metadata": {
        "id": "CFbD_ygKbFN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pointer\n",
        "# apmokomas modelis\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "for epoch in range(num_train_epochs):\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        outputs = model(**batch) # įkeliami duomenys dalimis\n",
        "        loss = outputs.loss # apskaičiuojamas loss\n",
        "        accelerator.backward(loss) # apskaičiuojamas gradientas\n",
        "        optimizer.step() # modelio parametrai keičiami pagal optimizatorių\n",
        "        lr_scheduler.step() # keičiamas learning rate\n",
        "        optimizer.zero_grad() # užnulina gradientą naujai iteracijai\n",
        "        progress_bar.update(1)\n",
        "# testuojamas modelis\n",
        "model.eval()\n",
        "for step, batch in enumerate(eval_dataloader):\n",
        "    # apskaičiuojamas rouge tik pirmiem 2 porom, kad pagreitinti testavimą\n",
        "    if step >= 1:\n",
        "        break\n",
        "    # atjungiamas gradient skaičiavimas\n",
        "    with torch.no_grad():\n",
        "        # generuojami spėjimai\n",
        "        generated_tokens = accelerator.unwrap_model(model).generate(\n",
        "            batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"],\n",
        "        )\n",
        "        # padding\n",
        "        generated_tokens = accelerator.pad_across_processes(\n",
        "            generated_tokens,\n",
        "            dim = 1,\n",
        "            pad_index = tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "        labels = batch[\"labels\"]\n",
        "        # padding\n",
        "        labels = accelerator.pad_across_processes(\n",
        "            batch[\"labels\"],\n",
        "            dim = 1,\n",
        "            pad_index = tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "        generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n",
        "        labels = accelerator.gather(labels).cpu().numpy()\n",
        "        # pakeisti -100 reikšmes, nes jų negalima dekoduoti\n",
        "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "        if isinstance(generated_tokens, tuple):\n",
        "            generated_tokens = generated_tokens[0]\n",
        "        # išversti tokenus į tekstą\n",
        "        decoded_preds = tokenizer.batch_decode(\n",
        "            generated_tokens, skip_special_tokens=True\n",
        "        )\n",
        "        # išversti tokenus į tekstą\n",
        "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "        # panaudojama ankstesnė funkcija\n",
        "        decoded_preds, decoded_labels = postprocess_text(\n",
        "            decoded_preds, decoded_labels\n",
        "        )\n",
        "        # apskaičuojamas rouge\n",
        "        rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
        "        result = rouge_score.compute()\n",
        "        result = {key: value for key, value in result.items()}\n",
        "        result = {key: round(value, 4) for key, value in result.items()}\n",
        "        print(f\"{model_name}:\", result)\n",
        "# išsaugomas modelis\n",
        "accelerator.wait_for_everyone()\n",
        "unwrapped_model = accelerator.unwrap_model(model)\n",
        "unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
        "if accelerator.is_main_process:\n",
        "    tokenizer.save_pretrained(output_dir)"
      ],
      "metadata": {
        "id": "_UzTrDaIbG4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tunned model testavimas ant naujų duomenų"
      ],
      "metadata": {
        "id": "V8aErCyqbcSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pointer\n",
        "\n",
        "# !gdown --fuzzy 'https://drive.google.com/file/d/1ftSBpUPsVtlswOzfc2HckbwMyx7GbrcY/view?usp=drive_link'\n",
        "# !unzip bart-base-finetuned.zip"
      ],
      "metadata": {
        "id": "ZmodMz-rgPL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "summarizer = pipeline(\"summarization\", model='bart-base-finetuned')\n",
        "\n",
        "def print_summary(text):\n",
        "    summary = summarizer(text)[0]['summary_text']\n",
        "    print(f\"> Text: {text}\\n\")\n",
        "    print(f\"> Summary: {summary}\")"
      ],
      "metadata": {
        "id": "wO5e8FggbcYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''\n",
        "Physicists in Italy and China have for the first time observed glimmers of the ‘neutrino fog’, signals from neutrinos that mimic those expected to be produced by dark matter.\n",
        "The observations are a double-edged sword, says Nicole Bell, a theoretical physicist at the University of Melbourne, Australia. On the one hand, it means that detectors have become sensitive enough to pick up signals of dark matter — the mysterious substance thought to make up the bulk of matter in the Universe. On the other, it means that the neutrino signals could obscure the dark-matter signals that scientists are so eager to observe. The findings were published in two papers in Physical Review Letters last month1,2.\n",
        "Every second, trillions of neutrinos stream through Earth — unnoticed, because they barely interact with ordinary matter. Most of these almost massless particles are produced by fusion reactions in the Sun, such as those that trigger the radioactive β-decay of the isotope boron-8.\n",
        "Physicists have long predicted that dark-matter experiments will eventually catch a glimpse of this neutrino fog, formerly known as the neutrino floor, says Fei Gao, an experimental particle physicist at Tsinghua University in Beijing. He works on the XENONnt dark-matter experiment at the Gran Sasso National Laboratory just outside L’Aquila, Italy.\n",
        "The neutrino fog is also exciting because measuring it confirms that dark-matter experiments are capable of observing all ‘flavours’ of neutrinos flying in from the Sun and even from exploding stars in nearby galaxies, says Kate Scholberg, an experimental particle physicist at Duke University in Durham, North Carolina. “You could learn something about the total spectrum of all neutrinos,” she says.\n",
        "'''\n",
        "print_summary(text)"
      ],
      "metadata": {
        "id": "MRT1lBt8beAE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}